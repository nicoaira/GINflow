#!/usr/bin/env python3
"""Merge per-chunk window vectors into consolidated artefacts.

The script consumes a manifest describing chunk outputs generated by the
``GENERATE_WINDOW_VECTORS`` process. For each chunk it expects:
    - database_windows.npy
    - database_windows.tsv
    - query_windows.npy
    - query_windows.tsv
    - window_stats.json

It concatenates the arrays along axis 0, merges the TSV metadata while
preserving a single header row, and writes a combined window_stats.json with
per-chunk provenance.
"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

import numpy as np


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Merge chunked window vector artefacts")
    parser.add_argument("--manifest", required=True, help="Tab-delimited manifest describing chunk outputs")
    parser.add_argument("--database-vectors", required=True, help="Output path for merged database window vectors (.npy)")
    parser.add_argument("--database-metadata", required=True, help="Output path for merged database metadata (.tsv)")
    parser.add_argument("--query-vectors", required=True, help="Output path for merged query window vectors (.npy)")
    parser.add_argument("--query-metadata", required=True, help="Output path for merged query metadata (.tsv)")
    parser.add_argument("--stats", required=True, help="Output path for aggregated window statistics JSON")
    parser.add_argument("--chunk-manifest", required=True, help="Output path to write the sorted chunk manifest TSV")
    return parser.parse_args()


def load_manifest(path: str) -> List[Dict[str, Any]]:
    manifest_path = Path(path)
    if not manifest_path.exists():
        raise SystemExit(f"Manifest not found: {manifest_path}")
    lines = [line.strip() for line in manifest_path.read_text().splitlines() if line.strip()]
    if not lines:
        return []
    header = lines[0].split('\t')
    expected = [
        "chunk_id",
        "database_windows",
        "database_metadata",
        "query_windows",
        "query_metadata",
        "window_stats",
    ]
    if header != expected:
        raise SystemExit(
            f"Unexpected manifest header {header}; expected {expected}"
        )
    data: List[Dict[str, Any]] = []
    for line in lines[1:]:
        parts = line.split('\t')
        if len(parts) != len(expected):
            raise SystemExit(f"Malformed manifest line: {line}")
        entry = dict(zip(expected, parts))
        data.append(entry)
    return data


def validate_entry(entry: Dict[str, Any], index: int) -> None:
    required = [
        "chunk_id",
        "database_windows",
        "database_metadata",
        "query_windows",
        "query_metadata",
        "window_stats",
    ]
    missing = [key for key in required if key not in entry]
    if missing:
        raise SystemExit(f"Manifest entry {index} missing keys: {', '.join(missing)}")


def append_metadata(src_path: Path, dest_handle, include_header: bool) -> None:
    with src_path.open("r", encoding="utf-8") as src:
        header = src.readline()
        if include_header and header:
            dest_handle.write(header)
        for line in src:
            dest_handle.write(line)


def infer_window_dim(path: Path) -> int:
    arr = np.load(path, mmap_mode="r")
    if arr.ndim == 0:
        raise SystemExit(f"Unable to infer window dimension from scalar array at {path}")
    if arr.ndim == 1:
        return int(arr.shape[0])
    return int(arr.shape[1])


def write_empty_array(path: Path, window_dim: int) -> None:
    empty = np.zeros((0, window_dim), dtype=np.float32)
    np.save(path, empty)


def main() -> None:
    args = parse_args()
    manifest = load_manifest(args.manifest)
    if not manifest:
        raise SystemExit("Manifest is empty; no window chunks to merge")

    chunks: List[Dict[str, Any]] = []
    for idx, entry in enumerate(manifest):
        validate_entry(entry, idx)
        chunk_id = str(entry["chunk_id"])
        stats_path = Path(entry["window_stats"])
        if not stats_path.exists():
            raise SystemExit(f"Stats JSON missing for chunk {chunk_id}: {stats_path}")
        try:
            stats = json.loads(stats_path.read_text())
        except json.JSONDecodeError as exc:
            raise SystemExit(f"Failed to parse stats for chunk {chunk_id}: {exc}")
        stats_chunk = stats.get("chunk_id")
        if stats_chunk and stats_chunk != chunk_id:
            raise SystemExit(
                f"Chunk id mismatch: manifest has '{chunk_id}' but stats reports '{stats_chunk}'"
            )
        chunks.append(
            {
                "chunk_id": chunk_id,
                "db_npy": Path(entry["database_windows"]),
                "db_tsv": Path(entry["database_metadata"]),
                "q_npy": Path(entry["query_windows"]),
                "q_tsv": Path(entry["query_metadata"]),
                "stats": stats,
                "stats_path": stats_path,
            }
        )

    chunks.sort(key=lambda item: item["chunk_id"])

    for chunk in chunks:
        if not chunk["db_npy"].exists():
            raise SystemExit(f"Missing database vector file for chunk {chunk['chunk_id']}: {chunk['db_npy']}")
        if not chunk["db_tsv"].exists():
            raise SystemExit(f"Missing database metadata TSV for chunk {chunk['chunk_id']}: {chunk['db_tsv']}")
        if not chunk["q_npy"].exists():
            raise SystemExit(f"Missing query vector file for chunk {chunk['chunk_id']}: {chunk['q_npy']}")
        if not chunk["q_tsv"].exists():
            raise SystemExit(f"Missing query metadata TSV for chunk {chunk['chunk_id']}: {chunk['q_tsv']}")

    window_dims = []
    for chunk in chunks:
        window_dims.append(infer_window_dim(chunk["db_npy"]))
    if len(set(window_dims)) > 1:
        raise SystemExit("Chunk window vector dimensions are inconsistent")
    window_dim = window_dims[0]

    db_total = sum(int(chunk["stats"].get("database_windows", 0)) for chunk in chunks)
    query_total = sum(int(chunk["stats"].get("query_windows", 0)) for chunk in chunks)

    db_output = Path(args.database_vectors)
    query_output = Path(args.query_vectors)

    db_mm = None
    if db_total > 0:
        db_mm = np.lib.format.open_memmap(db_output, mode="w+", dtype=np.float32, shape=(db_total, window_dim))
    else:
        write_empty_array(db_output, window_dim)

    query_mm = None
    if query_total > 0:
        query_mm = np.lib.format.open_memmap(query_output, mode="w+", dtype=np.float32, shape=(query_total, window_dim))
    else:
        write_empty_array(query_output, window_dim)

    db_cursor = 0
    query_cursor = 0

    db_metadata_path = Path(args.database_metadata)
    query_metadata_path = Path(args.query_metadata)

    with db_metadata_path.open("w", encoding="utf-8") as db_meta_handle, \
            query_metadata_path.open("w", encoding="utf-8") as query_meta_handle:
        header_written_db = False
        header_written_query = False

        for chunk in chunks:
            db_count = int(chunk["stats"].get("database_windows", 0))
            q_count = int(chunk["stats"].get("query_windows", 0))

            append_metadata(chunk["db_tsv"], db_meta_handle, include_header=not header_written_db)
            header_written_db = True

            append_metadata(chunk["q_tsv"], query_meta_handle, include_header=not header_written_query)
            header_written_query = True

            if db_mm is not None and db_count:
                chunk_db = np.load(chunk["db_npy"], mmap_mode="r")
                db_mm[db_cursor: db_cursor + db_count] = chunk_db[:db_count]
                db_cursor += db_count

            if query_mm is not None and q_count:
                chunk_query = np.load(chunk["q_npy"], mmap_mode="r")
                query_mm[query_cursor: query_cursor + q_count] = chunk_query[:q_count]
                query_cursor += q_count

    if db_mm is not None:
        db_mm.flush()
        del db_mm
    if query_mm is not None:
        query_mm.flush()
        del query_mm

    if db_cursor != db_total:
        raise SystemExit(
            f"Database window count mismatch: expected {db_total}, wrote {db_cursor}"
        )
    if query_cursor != query_total:
        raise SystemExit(
            f"Query window count mismatch: expected {query_total}, wrote {query_cursor}"
        )

    window_sizes = {chunk["stats"].get("window_size") for chunk in chunks if "window_size" in chunk["stats"]}
    strides = {chunk["stats"].get("stride") for chunk in chunks if "stride" in chunk["stats"]}
    if len(window_sizes) > 1:
        raise SystemExit("Inconsistent window_size observed across chunks")
    if len(strides) > 1:
        raise SystemExit("Inconsistent stride observed across chunks")

    summary = {
        "database_windows": db_total,
        "query_windows": query_total,
        "window_dim": window_dim,
        "window_size": next(iter(window_sizes)) if window_sizes else None,
        "stride": next(iter(strides)) if strides else None,
        "chunks": len(chunks),
        "chunk_ids": [chunk["chunk_id"] for chunk in chunks],
        "queries": sum(int(chunk["stats"].get("queries", 0)) for chunk in chunks),
        "transcripts": sum(int(chunk["stats"].get("transcripts", 0)) for chunk in chunks),
        "chunk_stats": {chunk["chunk_id"]: chunk["stats"] for chunk in chunks},
    }

    stats_path = Path(args.stats)
    stats_path.write_text(json.dumps(summary, indent=2))

    manifest_lines = [
        "chunk_id\tdatabase_windows\tdatabase_metadata\tquery_windows\tquery_metadata\twindow_stats"
    ]
    for chunk in chunks:
        manifest_lines.append(
            "\t".join(
                [
                    chunk["chunk_id"],
                    str(chunk["db_npy"]),
                    str(chunk["db_tsv"]),
                    str(chunk["q_npy"]),
                    str(chunk["q_tsv"]),
                    str(chunk["stats_path"]),
                ]
            )
        )
    Path(args.chunk_manifest).write_text("\n".join(manifest_lines) + "\n")


if __name__ == "__main__":
    main()
