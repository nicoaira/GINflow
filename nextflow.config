/*
 * nextflow.config – Default pipeline parameters
 */

////////////////////////////////////////////////////////////////////////////////
// Global params block
////////////////////////////////////////////////////////////////////////////////
params {
    // ── Input/output ────────────────────────────────────────────────────────
    input                       = null
    outdir                      = 'results'
    header                      = true
    keep_cols                   = 'transcript_id'
    id_column                   = 'transcript_id'
    split_size                  = 1024

    // ── Main-workflow control ───────────────────────────────────────────────
    subgraphs                   = true
    L                           = 30
    keep_paired_neighbors       = true
    structure_column_name       = 'secondary_structure'
    structure_column_num        = null
    mask_threshold              = 0.3

    // ── Embedding generation ────────────────────────────────────────────────
    num_workers                 = 4
    inference_batch_size        = 4096

    // GPU usage flag (overridden by the `gpu` profile)
    use_gpu                     = false

    // ── FAISS index/query ───────────────────────────────────────────────────
    faiss_k                     = 1000
    query                       = null
    top_n                       = 10

    // ── Aggregation parameters ──────────────────────────────────────────────
    alpha1                      = 0.25
    alpha2                      = 0.24
    beta1                       = 0.0057
    beta2                       = 1.15
    gamma                       = 0.41
    percentile                  = 1

    // ── Plotting ────────────────────────────────────────────────────────────
    plot_distances_distribution = true
    hist_seed                   = 42
    hist_frac                   = 0.001
    hist_bins                   = 200
    plot_score_distribution     = true
    score_bins                  = 30

    // ── Report generation ───────────────────────────────────────────────────
    run_aggregated_report       = true
    run_unaggregated_report     = false
}

////////////////////////////////////////////////////////////////////////////////
// Profiles – can be combined, e.g. `-profile test,gpu,docker,slurm`
////////////////////////////////////////////////////////////////////////////////
profiles {

    /*
     * Small test dataset + minimal resources.
     */
    test {
        params {
            input                       = "${baseDir}/tests/data/test_input.tsv"
            outdir                      = 'test_results'

            split_size                  = 100
            num_workers                 = 2
            inference_batch_size        = 1000
            faiss_k                     = 1000
            top_n                       = 5

            // Use first transcript as query for testing
            query                       = 'ENST00000832823.1'

            // Disable some plots for faster testing
            plot_distances_distribution = true
            plot_score_distribution     = true
        }
    }

    /*
     * Local monitoring profile with detailed resource tracking
     */
    monitor {
        // Don't override executor - let other profiles handle that
        // Just enable detailed reporting
        params.enable_monitoring = true
    }

    /*
     * Explicit local executor (mostly for clarity when combining profiles).
     */
    local {
        process.executor = 'local'
    }

    /*
     * SLURM cluster executor.
     * Extend with clusterOptions/queue/time limits as needed.
     */
     
    slurm {
        process {
            executor = 'slurm'

            // generic slurm defaults
            queue    = 'short' 
            cpus     = 2
            time     = '4h'
            
            clusterOptions = { ["--job-name=" + task.process + "_" + task.index] }


            withLabel: 'gpu' {
                queue          = 'viz'
                clusterOptions =  {
                [
                    "--gres=gpu:t4:1",
                    "--job-name=" + task.process + "_" + task.index,
                    "--qos=viz"
                ]
                }

                cpus           = 8
                memory         = '32 GB'
                time           = '12m'
                maxForks       = 2
            }

            withLabel: 'medium_memory' {
                queue          = 'medium'
                cpus           = 4
                memory         = '64 GB'
                time           = '30m'

                errorStrategy = 'retry'
                maxRetries    = 2
                memory = { 16 * task.attempt.GB }
            }

            withLabel: 'high_memory' {
                queue          = 'medium' 
                cpus          = 4
                time          = '30m'
                errorStrategy = 'retry'
                maxRetries    = 2

                memory = { 64 * task.attempt.GB }
            }

            withLabel: 'high_cpu' {
                queue          = 'medium' 
                cpus           = { 16 * task.attempt }
                memory         = '32 GB'
                time           = '30m'
                maxRetries     = 2
            }

            withLabel: 'lightweight' {
                queue          = 'short'
                cpus           = 1
                memory         = '8 GB'
                time           = '30m'
            }
        }
    }

    /*
     * Enable GPU support.
     * Sets `params.use_gpu` so other profiles (e.g. `docker`) can react.
     */
    gpu {
        params.use_gpu = true
    }

    /*
     * Software-stack profiles
     * (remain orthogonal; combine freely with the above).
     */
    conda {
        conda.enabled       = true
        docker.enabled      = false
        singularity.enabled = false
        conda.channels      = ['conda-forge', 'bioconda']
    }

    mamba {
        conda.enabled       = true
        conda.useMamba      = true
        docker.enabled      = false
        singularity.enabled = false
    }

    docker {
        docker.enabled      = true
        conda.enabled       = false
        singularity.enabled = false
        // If `gpu` profile is also active, expose all GPUs;
        // otherwise fall back to running as the current UID/GID.
        docker.runOptions   = params.use_gpu ? '--gpus all' : '-u $(id -u):$(id -g)'
    }

    singularity {
        singularity.enabled = true
        singularity.autoMounts = true
        conda.enabled       = false
        docker.enabled      = false
        singularity.runOptions = params.use_gpu ? '--nv' : ''
    }
}

////////////////////////////////////////////////////////////////////////////////
// Execution reports – enable for resource monitoring
////////////////////////////////////////////////////////////////////////////////

trace {
    enabled = true
    file    = "${params.outdir}/reports/trace.tsv"
    overwrite = true
    fields  = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

report {
    enabled = true
    file    = "${params.outdir}/reports/report.html"
    overwrite = true
}

timeline {
    enabled = true
    file    = "${params.outdir}/reports/timeline.html"
    overwrite = true
}

dag {
    enabled = true
    file    = "${params.outdir}/reports/dag.svg"
    overwrite = true
}

////////////////////////////////////////////////////////////////////////////////
// Pipeline manifest
////////////////////////////////////////////////////////////////////////////////
manifest {
    name       = 'GINflow'
    version    = '0.1.0'
    mainScript = 'main.nf'
}
