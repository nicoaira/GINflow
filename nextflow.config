/*
 * nextflow.config – Default pipeline parameters
 */

////////////////////////////////////////////////////////////////////////////////
// Global params block
////////////////////////////////////////////////////////////////////////////////
params {
    // ── Input/output ────────────────────────────────────────────────────────
    input                       = null
    outdir                      = 'results'
    header                      = true
    keep_cols                   = 'transcript_id'
    id_column                   = 'transcript_id'
    split_size                  = 1024

    // ── Main-workflow control ───────────────────────────────────────────────
    subgraphs                   = true
    L                           = 30
    keep_paired_neighbors       = true
    structure_column_name       = 'secondary_structure'
    structure_column_num        = null
    mask_threshold              = 0.3

    // ── Embedding generation ────────────────────────────────────────────────
    num_workers                 = 4
    inference_batch_size        = 4096

    // GPU usage flag (overridden by the `gpu` profile)
    use_gpu                     = false
    gpu_type                    = 't4' // Or 'a100'

    // ── FAISS index/query ───────────────────────────────────────────────────
    faiss_k                     = 1000
    queries                     = null
    top_n                       = 10
    embeddings_tsv              = null
    faiss_index                 = null
    faiss_mapping               = null

    // ── Aggregation parameters ──────────────────────────────────────────────
    alpha1                      = 0.25
    alpha2                      = 0.24
    beta1                       = 0.0057
    beta2                       = 1.15
    gamma                       = 0.41
    percentile                  = 1

    // Path to a TSV file containing a 'score' column representing the null
    // distribution. If provided, merged contig scores will be normalized to
    // produce z-scores and p-values.
    null_scores                 = null
    // Number of dinucleotide shuffles per query to generate a null
    // distribution automatically. If > 0 and --null_scores is not supplied,
    // shuffled sequences will be folded with RNAfold to create baseline scores.
    null_iterations             = 0

    // ── Plotting ────────────────────────────────────────────────────────────
    plot_distances_distribution = true
    hist_seed                   = 42
    hist_frac                   = 0.001
    hist_bins                   = 200
    plot_score_distribution     = true
    score_bins                  = 30

    // ── Report generation ───────────────────────────────────────────────────
    draw_contig_svgs            = true
    run_aggregated_report       = true
    run_unaggregated_report     = false
}

////////////////////////////////////////////////////////////////////////////////
// Profiles – can be combined, e.g. `-profile test,gpu,docker,slurm`
////////////////////////////////////////////////////////////////////////////////
profiles {

    /*
     * Ultra-fast smoke test dataset (runs in < ~1-2 min) for agent branches (codex/*).
     * Derived from `test` but with reduced k/top_n and batch sizes for speed.
     */
    smoke {
        params {
            input                       = "${baseDir}/tests/data/test_input.tsv"
            outdir                      = 'smoke_results'
            split_size                  = 50
            num_workers                 = 1
            inference_batch_size        = 256
            faiss_k                     = 100
            top_n                       = 3
            queries                     = "${baseDir}/tests/data/test_queries.csv"
            plot_distances_distribution = false
            plot_score_distribution     = false
            draw_contig_svgs            = false
            run_aggregated_report       = true
            run_unaggregated_report     = false
        }
    }

    /*
     * Small test dataset + minimal resources.
     */
    test {
        params {
            input                       = "${baseDir}/tests/data/test_input.tsv"
            outdir                      = 'test_results'

            split_size                  = 100
            num_workers                 = 2
            inference_batch_size        = 1000
            faiss_k                     = 1000
            top_n                       = 5

            // Use a small set of transcript IDs for testing
            queries                     = "${baseDir}/tests/data/test_queries.csv"

            // Disable some plots for faster testing
            plot_distances_distribution = true
            plot_score_distribution     = true
        }
    }

    /*
     * Local monitoring profile with detailed resource tracking
     */
    monitor {
        // Don't override executor - let other profiles handle that
        // Just enable detailed reporting
        params.enable_monitoring = true
    }

    /*
     * Explicit local executor (mostly for clarity when combining profiles).
     */
    local {
        process.executor = 'local'
    }

    /*
     * SLURM cluster executor.
     * Extend with clusterOptions/queue/time limits as needed.
     */
     
    slurm {
        process {
            executor = 'slurm'

            // generic slurm defaults
            cpus     = 2
            time     = '4h'
            
            clusterOptions = { ["--job-name=" + task.process + "_" + task.index] }

            withLabel: 'gpu' {
                clusterOptions = {
                    def gres = params.gpu_type == 'a100' ? 'gpu:a100:1' : 'gpu:t4:1'
                    def qos = params.gpu_type == 'a100' ? 'class_a' : 'viz'
                    def partition = params.gpu_type == 'a100' ? 'short' : 'viz'
                    def opts = [
                        "--gres=${gres}".toString(),
                        "--job-name=${task.process}_${task.index}".toString(),
                        "--qos=${qos}".toString(),
                        "--partition=${partition}".toString()
                    ]
                    opts
                }
                cpus           = params.gpu_type == 'a100' ? 32 : 8
                memory         = '32 GB'
                time           = params.gpu_type == 'a100' ? '3m' : '12m'
                maxForks       = params.gpu_type == 'a100' ? 24 : 2
            }

            withLabel: 'medium_memory' {
                cpus           = 4
                memory         = '64 GB'
                time           = '30m'

                errorStrategy  = 'retry'
                maxRetries     = 2
                memory         = { 16 * task.attempt + ' GB' }
            }

            withLabel: 'high_memory' {
                cpus           = 4
                time           = '30m'
                errorStrategy  = 'retry'
                maxRetries     = 2
                memory         = { 64 * task.attempt + ' GB' }
            }

            withLabel: 'high_cpu' {
                cpus           = { 16 * task.attempt }
                memory         = '32 GB'
                time           = '30m'
                maxRetries     = 2
            }

            withLabel: 'lightweight' {
                cpus           = 1
                memory         = '8 GB'
                time           = '30m'
            }
        }
    }

    /*
     * Enable GPU support.
     * Sets `params.use_gpu` so other profiles (e.g. `docker`) can react.
     */
    gpu {
        params.use_gpu = true
    }

    /*
     * Software-stack profiles
     * (remain orthogonal; combine freely with the above).
     */
    conda {
        conda.enabled       = true
        docker.enabled      = false
        singularity.enabled = false
        conda.channels      = ['conda-forge', 'bioconda']
    }

    mamba {
        conda.enabled       = true
        conda.useMamba      = true
        docker.enabled      = false
        singularity.enabled = false
    }

    docker {
        docker.enabled      = true
        conda.enabled       = false
        singularity.enabled = false
        // If `gpu` profile is also active, expose all GPUs;
        // otherwise fall back to running as the current UID/GID.
        docker.runOptions   = params.use_gpu ? '--gpus all' : '-u $(id -u):$(id -g)'
    }

    singularity {
        singularity.enabled = true
        singularity.autoMounts = true
        conda.enabled       = false
        docker.enabled      = false
        singularity.runOptions = params.use_gpu ? '--nv' : ''
    }
}

////////////////////////////////////////////////////////////////////////////////
// Execution reports – enable for resource monitoring
////////////////////////////////////////////////////////////////////////////////

trace {
    enabled = true
    file    = "${params.outdir}/reports/trace.tsv"
    overwrite = true
    fields  = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

report {
    enabled = true
    file    = "${params.outdir}/reports/report.html"
    overwrite = true
}

timeline {
    enabled = true
    file    = "${params.outdir}/reports/timeline.html"
    overwrite = true
}

dag {
    enabled = true
    file    = "${params.outdir}/reports/dag.svg"
    overwrite = true
}

////////////////////////////////////////////////////////////////////////////////
// Pipeline manifest
////////////////////////////////////////////////////////////////////////////////
manifest {
    name       = 'GINflow'
    version    = '0.1.0'
    mainScript = 'main.nf'
}
