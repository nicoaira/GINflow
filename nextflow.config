/*
 * nextflow.config – Default pipeline parameters
 */

////////////////////////////////////////////////////////////////////////////////
// Global params block
////////////////////////////////////////////////////////////////////////////////
params {
    // ── Input/output ────────────────────────────────────────────────────────
    input                       = null
    outdir                      = 'results'
    header                      = true
    keep_cols                   = 'transcript_id'
    id_column                   = 'transcript_id'

    // ── Main-workflow control ───────────────────────────────────────────────
    subgraphs                   = true
    L                           = 30
    keep_paired_neighbors       = true
    structure_column_name       = 'secondary_structure'
    structure_column_num        = null
    mask_threshold              = 0.3

    // ── Embedding generation ────────────────────────────────────────────────
    num_workers                 = 4
    batch_size                  = 4096
    batch_size_embed            = 128

    // GPU usage flag (overridden by the `gpu` profile)
    use_gpu                     = false

    // ── FAISS index/query ───────────────────────────────────────────────────
    faiss_k                     = 1000
    query                       = null
    top_n                       = 10

    // ── Aggregation parameters ──────────────────────────────────────────────
    alpha1                      = 0.25
    alpha2                      = 0.24
    beta1                       = 0.0057
    beta2                       = 1.15
    gamma                       = 0.41
    percentile                  = 1

    // ── Plotting ────────────────────────────────────────────────────────────
    plot_distances_distribution = true
    hist_seed                   = 42
    hist_frac                   = 0.001
    hist_bins                   = 200
    plot_score_distribution     = true
    score_bins                  = 30
}

////////////////////////////////////////////////////////////////////////////////
// Profiles – can be combined, e.g. `-profile test,gpu,docker,slurm`
////////////////////////////////////////////////////////////////////////////////
profiles {

    /*
     * Small test dataset + minimal resources.
     */
    test {
        params {
            input                       = "${baseDir}/tests/data/test_input.tsv"
            outdir                      = 'test_results'

            batch_size_embed            = 100
            num_workers                 = 2
            batch_size                  = 1000
            faiss_k                     = 1000
            top_n                       = 5

            // Use first transcript as query for testing
            query                       = 'ENST00000832823.1'

            // Disable some plots for faster testing
            plot_distances_distribution = true
            plot_score_distribution     = true
        }
    }

    /*
     * Explicit local executor (mostly for clarity when combining profiles).
     */
    local {
        process.executor = 'local'
    }

    /*
     * SLURM cluster executor.
     * Extend with clusterOptions/queue/time limits as needed.
     */
     
    slurm {
            process.executor = 'slurm'
            // generic slurm defaults (edit to suit your cluster)
            process.queue    = 'compute'
            process.cpus     = 2
            process.time     = '4h'

            /* EXTRA GPU RESOURCES – applied only to tasks labelled 'gpu' */
            process {
                withLabel: 'gpu' {
                    clusterOptions = '--gres=gpu:1'  // Slurm asks for 1 GPU
                    queue          = 'gpu'           // send to GPU partition
                    cpus           = 4               // override if you wish
                    memory         = '16 GB'
                    time           = '24h'
                }
            }
        }

    /*
     * Enable GPU support.
     * Sets `params.use_gpu` so other profiles (e.g. `docker`) can react.
     */
    gpu {
        params.use_gpu = true
    }

    /*
     * Software-stack profiles
     * (remain orthogonal; combine freely with the above).
     */
    conda {
        conda.enabled       = true
        docker.enabled      = false
        singularity.enabled = false
        conda.channels      = ['conda-forge', 'bioconda']
    }

    mamba {
        conda.enabled       = true
        conda.useMamba      = true
        docker.enabled      = false
        singularity.enabled = false
    }

    docker {
        docker.enabled      = true
        conda.enabled       = false
        singularity.enabled = false
        // If `gpu` profile is also active, expose all GPUs;
        // otherwise fall back to running as the current UID/GID.
        docker.runOptions   = params.use_gpu ? '--gpus all' : '-u $(id -u):$(id -g)'
    }

    singularity {
        singularity.enabled = true
        singularity.autoMounts = true
        conda.enabled       = false
        docker.enabled      = false
    }
}

////////////////////////////////////////////////////////////////////////////////
// (Optional) Execution reports – enable as needed
////////////////////////////////////////////////////////////////////////////////
/*
trace {
  enabled = true
  file    = "${reportsDir}/trace.tsv"
}

report {
  enabled = true
  file    = "${reportsDir}/report.html"
}

timeline {
  enabled = true
  file    = "${reportsDir}/timeline.html"
}

dag {
  enabled = true
  file    = "${reportsDir}/flowchart.png"
}
*/

////////////////////////////////////////////////////////////////////////////////
// Pipeline manifest
////////////////////////////////////////////////////////////////////////////////
manifest {
    name       = 'GINflow'
    version    = '0.1.0'
    mainScript = 'main.nf'
}
