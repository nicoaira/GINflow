/*
 * nextflow.config – Default pipeline parameters
 */

////////////////////////////////////////////////////////////////////////////////
// Global params block
////////////////////////////////////////////////////////////////////////////////
params {
    // ── Input/output ────────────────────────────────────────────────────────
    input                       = null
    outdir                      = 'results'
    header                      = true
    keep_cols                   = 'transcript_id'
    id_column                   = 'transcript_id'
    split_size            = 128

    // ── Main-workflow control ───────────────────────────────────────────────
    subgraphs                   = true
    L                           = 30
    keep_paired_neighbors       = true
    structure_column_name       = 'secondary_structure'
    structure_column_num        = null
    mask_threshold              = 0.3

    // ── Embedding generation ────────────────────────────────────────────────
    num_workers                 = 4
    inference_batch_size        = 4096

    // GPU usage flag (overridden by the `gpu` profile)
    use_gpu                     = false

    // ── FAISS index/query ───────────────────────────────────────────────────
    faiss_k                     = 1000
    query                       = null
    top_n                       = 10

    // ── Aggregation parameters ──────────────────────────────────────────────
    alpha1                      = 0.25
    alpha2                      = 0.24
    beta1                       = 0.0057
    beta2                       = 1.15
    gamma                       = 0.41
    percentile                  = 1

    // ── Plotting ────────────────────────────────────────────────────────────
    plot_distances_distribution = true
    hist_seed                   = 42
    hist_frac                   = 0.001
    hist_bins                   = 200
    plot_score_distribution     = true
    score_bins                  = 30

    // ── Report generation ───────────────────────────────────────────────────
    run_aggregated_report       = true
    run_unaggregated_report     = false
}

////////////////////////////////////////////////////////////////////////////////
// Profiles – can be combined, e.g. `-profile test,gpu,docker,slurm`
////////////////////////////////////////////////////////////////////////////////
profiles {

    /*
     * Small test dataset + minimal resources.
     */
    test {
        params {
            input                       = "${baseDir}/tests/data/test_input.tsv"
            outdir                      = 'test_results'

            split_size            = 100
            num_workers                 = 2
            inference_batch_size                  = 1000
            faiss_k                     = 1000
            top_n                       = 5

            // Use first transcript as query for testing
            query                       = 'ENST00000832823.1'

            // Disable some plots for faster testing
            plot_distances_distribution = true
            plot_score_distribution     = true
        }
    }

    /*
     * Local monitoring profile with detailed resource tracking
     */
    monitor {
        // Don't override executor - let other profiles handle that
        // Just enable detailed reporting
        params.enable_monitoring = true
    }

    /*
     * Explicit local executor (mostly for clarity when combining profiles).
     */
    local {
        process.executor = 'local'
    }

    /*
     * SLURM cluster executor.
     * Extend with clusterOptions/queue/time limits as needed.
     */
     
    slurm {
        process.executor = 'slurm'
        // generic slurm defaults (edit to suit your cluster)
        process.queue    = 'compute'
        process.cpus     = 2
        process.time     = '4h'

        /* EXTRA GPU RESOURCES – applied only to tasks labelled 'gpu' */
        process {
            withLabel: 'gpu' {
                clusterOptions = '--gres=gpu:1'  // Slurm asks for 1 GPU
                queue          = 'gpu'           // send to GPU partition
                cpus           = 8               
                memory         = '32 GB'         
                time           = '30m'
            }
            
            withLabel: 'medium_memory' {
                queue          = 'highmem'
                cpus           = 4
                memory         = '64 GB'
                time           = '30m'
            }

            withLabel: 'high_memory' {
                queue          = 'highmem'
                cpus           = 4
                memory         = '152 GB'
                time           = '30m'
            }
            
            withLabel: 'high_cpu' {
                queue          = 'compute'
                cpus           = 16
                memory         = '16 GB'
                time           = '30m'
            }
            
            withLabel: 'lightweight' {
                queue          = 'express'
                cpus           = 1
                memory         = '8 GB'
                time           = '30m'
            }
            
            // Process-specific overrides based on expected resource usage
            withName: 'rna_similarity:GENERATE_EMBEDDINGS' {
                cpus           = 8
                memory         = '16 GB'
                time           = '6h'
                clusterOptions = '--gres=gpu:1'
                queue          = 'gpu'
            }
            
            withName: 'rna_similarity:BUILD_FAISS_INDEX' {
                cpus           = 16
                memory         = '64 GB'
                time           = '4h'
                queue          = 'highmem'
            }
            
            withName: 'rna_similarity:QUERY_FAISS_INDEX' {
                cpus           = 8
                memory         = '32 GB'
                time           = '3h'
            }
            
            withName: 'rna_similarity:GENERATE_WINDOWS' {
                cpus           = 4
                memory         = '8 GB'
                time           = '2h'
            }
            
            withName: 'rna_similarity:MERGE_EMBEDDINGS' {
                cpus           = 2
                memory         = '8 GB'
                time           = '1h'
            }
            
            withName: 'rna_similarity:EXTRACT_META_MAP' {
                cpus           = 1
                memory         = '4 GB'
                time           = '30m'
                queue          = 'express'
            }
            
            // Error retry with more resources
            errorStrategy = 'retry'
            maxRetries    = 2
            
            // Dynamic resource allocation - increases resources on retry
            cpus   = { 2 * task.attempt }
            memory = { 8.GB * task.attempt }
            time   = { 4.h * task.attempt }
        }
    }

    /*
     * Enable GPU support.
     * Sets `params.use_gpu` so other profiles (e.g. `docker`) can react.
     */
    gpu {
        params.use_gpu = true
    }

    /*
     * Software-stack profiles
     * (remain orthogonal; combine freely with the above).
     */
    conda {
        conda.enabled       = true
        docker.enabled      = false
        singularity.enabled = false
        conda.channels      = ['conda-forge', 'bioconda']
    }

    mamba {
        conda.enabled       = true
        conda.useMamba      = true
        docker.enabled      = false
        singularity.enabled = false
    }

    docker {
        docker.enabled      = true
        conda.enabled       = false
        singularity.enabled = false
        // If `gpu` profile is also active, expose all GPUs;
        // otherwise fall back to running as the current UID/GID.
        docker.runOptions   = params.use_gpu ? '--gpus all' : '-u $(id -u):$(id -g)'
    }

    singularity {
        singularity.enabled = true
        singularity.autoMounts = true
        conda.enabled       = false
        docker.enabled      = false
    }
}

////////////////////////////////////////////////////////////////////////////////
// Execution reports – enable for resource monitoring
////////////////////////////////////////////////////////////////////////////////

trace {
    enabled = true
    file    = "${params.outdir}/reports/trace.tsv"
    overwrite = true
    fields  = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

report {
    enabled = true
    file    = "${params.outdir}/reports/report.html"
    overwrite = true
}

timeline {
    enabled = true
    file    = "${params.outdir}/reports/timeline.html"
    overwrite = true
}

dag {
    enabled = true
    file    = "${params.outdir}/reports/dag.svg"
    overwrite = true
}

////////////////////////////////////////////////////////////////////////////////
// Pipeline manifest
////////////////////////////////////////////////////////////////////////////////
manifest {
    name       = 'GINflow'
    version    = '0.1.0'
    mainScript = 'main.nf'
}
